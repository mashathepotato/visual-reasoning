{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN Rotation Alignment Agent (Kornia + PyTorch)\n",
        "\n",
        "This notebook implements a DQN agent that learns to align a source image to a target image via rotation, or declare a mismatch when the images are fundamentally different (for example mirrored). The environment uses kornia for differentiable GPU-accelerated rotation and keeps all rotation ops on the active device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Callable, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import kornia as K\n",
        "import gymnasium as gym\n",
        "\n",
        "try:\n",
        "    import imageio.v2 as imageio\n",
        "except Exception:\n",
        "    import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "def get_device() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 0) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def build_state(source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    # Build 6-channel state by concatenating source and target.\n",
        "    # Handles batch expansion of target if needed.\n",
        "    if source.dim() == 3:\n",
        "        source = source.unsqueeze(0)\n",
        "    if target.dim() == 3:\n",
        "        target = target.unsqueeze(0)\n",
        "    if target.shape[0] == 1 and source.shape[0] > 1:\n",
        "        target = target.expand(source.shape[0], -1, -1, -1)\n",
        "    return torch.cat([source, target], dim=1)\n",
        "\n",
        "\n",
        "def rotate_tensor(img: torch.Tensor, angle_deg) -> torch.Tensor:\n",
        "    # Rotate image(s) by angle(s) in degrees using kornia.\n",
        "    if img.dim() == 3:\n",
        "        img_b = img.unsqueeze(0)\n",
        "    else:\n",
        "        img_b = img\n",
        "\n",
        "    if not torch.is_tensor(angle_deg):\n",
        "        angle = torch.tensor([angle_deg], device=img_b.device, dtype=img_b.dtype)\n",
        "    else:\n",
        "        angle = angle_deg.to(device=img_b.device, dtype=img_b.dtype)\n",
        "\n",
        "    if angle.dim() == 0:\n",
        "        angle = angle.unsqueeze(0)\n",
        "    if angle.numel() == 1 and img_b.shape[0] > 1:\n",
        "        angle = angle.repeat(img_b.shape[0])\n",
        "\n",
        "    rotated = K.geometry.transform.rotate(img_b, angle)\n",
        "    return rotated if img.dim() == 4 else rotated.squeeze(0)\n",
        "\n",
        "\n",
        "device = get_device()\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RotationEnv(gym.Env):\n",
        "    # Observation: (6, H, W) = [source(3), target(3)] in [0, 1], torch tensor on device.\n",
        "    # Actions:\n",
        "    #   0: rotate CW  (delta = -5 deg)\n",
        "    #   1: rotate CCW (delta = +5 deg)\n",
        "    #   2: commit match\n",
        "    #   3: commit mismatch\n",
        "\n",
        "    metadata = {\"render_modes\": []}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_shape: Tuple[int, int, int],\n",
        "        max_steps: int = 50,\n",
        "        epsilon: float = 0.01,\n",
        "        step_penalty: float = -0.1,\n",
        "        device: Optional[torch.device] = None,\n",
        "        pair_sampler: Optional[Callable[[torch.device], Tuple[torch.Tensor, torch.Tensor, bool]]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_shape = image_shape  # (3, H, W)\n",
        "        self.max_steps = max_steps\n",
        "        self.epsilon = epsilon\n",
        "        self.step_penalty = step_penalty\n",
        "        self.device = device or get_device()\n",
        "        self.pair_sampler = pair_sampler\n",
        "\n",
        "        _, h, w = image_shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(6, h, w),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        self.base_source = None\n",
        "        self.target = None\n",
        "        self.current_source = None\n",
        "        self.current_angle = 0.0\n",
        "        self.is_mirrored = False\n",
        "        self.step_count = 0\n",
        "\n",
        "    def _set_pair(self, source: torch.Tensor, target: torch.Tensor, is_mirrored: bool) -> None:\n",
        "        self.base_source = source.to(self.device).float().clamp(0, 1)\n",
        "        self.target = target.to(self.device).float().clamp(0, 1)\n",
        "        self.is_mirrored = bool(is_mirrored)\n",
        "        self.current_angle = 0.0\n",
        "        self.current_source = self.base_source\n",
        "        self.step_count = 0\n",
        "\n",
        "    def _obs(self) -> torch.Tensor:\n",
        "        return build_state(self.current_source, self.target)[0]\n",
        "\n",
        "    def _alignment_error(self) -> torch.Tensor:\n",
        "        return torch.mean((self.current_source - self.target) ** 2)\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        options = options or {}\n",
        "\n",
        "        if \"pair\" in options:\n",
        "            source, target, is_mirrored = options[\"pair\"]\n",
        "        elif \"source\" in options and \"target\" in options:\n",
        "            source = options[\"source\"]\n",
        "            target = options[\"target\"]\n",
        "            is_mirrored = bool(options.get(\"is_mirrored\", False))\n",
        "        elif self.pair_sampler is not None:\n",
        "            source, target, is_mirrored = self.pair_sampler(self.device)\n",
        "        else:\n",
        "            raise ValueError(\"No pair provided. Pass options={pair: (...)} or set pair_sampler.\")\n",
        "\n",
        "        self._set_pair(source, target, is_mirrored)\n",
        "        info = {\"angle\": self.current_angle}\n",
        "        return self._obs(), info\n",
        "\n",
        "    def step(self, action: int):\n",
        "        terminated = False\n",
        "        reward = 0.0\n",
        "\n",
        "        if action == 0 or action == 1:\n",
        "            delta = -5.0 if action == 0 else 5.0\n",
        "            self.current_angle = (self.current_angle + delta) % 360.0\n",
        "            self.current_source = rotate_tensor(self.base_source, self.current_angle)\n",
        "            reward = self.step_penalty\n",
        "        elif action == 2:\n",
        "            err = self._alignment_error()\n",
        "            is_match = (not self.is_mirrored) and (err < self.epsilon)\n",
        "            reward = 100.0 if is_match else -100.0\n",
        "            terminated = True\n",
        "        elif action == 3:\n",
        "            reward = 100.0 if self.is_mirrored else -100.0\n",
        "            terminated = True\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action: {action}\")\n",
        "\n",
        "        self.step_count += 1\n",
        "        truncated = (self.step_count >= self.max_steps) and (not terminated)\n",
        "\n",
        "        info = {\n",
        "            \"angle\": self.current_angle,\n",
        "            \"error\": float(self._alignment_error().detach().cpu()),\n",
        "            \"is_mirrored\": self.is_mirrored,\n",
        "        }\n",
        "        return self._obs(), float(reward), terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, int, int], num_actions: int = 4):\n",
        "        super().__init__()\n",
        "        c, h, w = input_shape\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(c, 32, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, c, h, w)\n",
        "            out = self.features(dummy)\n",
        "            conv_out = out.view(1, -1).shape[1]\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(conv_out, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int, state_shape: Tuple[int, int, int], store_uint8: bool = True):\n",
        "        self.capacity = capacity\n",
        "        self.state_shape = state_shape\n",
        "        self.store_uint8 = store_uint8\n",
        "\n",
        "        state_dtype = torch.uint8 if store_uint8 else torch.float32\n",
        "        self.states = torch.empty((capacity, *state_shape), dtype=state_dtype)\n",
        "        self.next_states = torch.empty((capacity, *state_shape), dtype=state_dtype)\n",
        "        self.actions = torch.empty((capacity,), dtype=torch.int64)\n",
        "        self.rewards = torch.empty((capacity,), dtype=torch.float32)\n",
        "        self.dones = torch.empty((capacity,), dtype=torch.uint8)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def push(self, state: torch.Tensor, action: int, reward: float, next_state: torch.Tensor, done: bool) -> None:\n",
        "        if self.store_uint8:\n",
        "            s = (state.detach().cpu().clamp(0, 1) * 255).to(torch.uint8)\n",
        "            ns = (next_state.detach().cpu().clamp(0, 1) * 255).to(torch.uint8)\n",
        "        else:\n",
        "            s = state.detach().cpu().float()\n",
        "            ns = next_state.detach().cpu().float()\n",
        "\n",
        "        self.states[self.idx] = s\n",
        "        self.next_states[self.idx] = ns\n",
        "        self.actions[self.idx] = int(action)\n",
        "        self.rewards[self.idx] = float(reward)\n",
        "        self.dones[self.idx] = int(done)\n",
        "\n",
        "        self.idx = (self.idx + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def sample(self, batch_size: int, device: torch.device):\n",
        "        idx = torch.randint(0, self.size, (batch_size,))\n",
        "        states = self.states[idx].to(device)\n",
        "        next_states = self.next_states[idx].to(device)\n",
        "        if self.store_uint8:\n",
        "            states = states.float() / 255.0\n",
        "            next_states = next_states.float() / 255.0\n",
        "        actions = self.actions[idx].to(device)\n",
        "        rewards = self.rewards[idx].to(device)\n",
        "        dones = self.dones[idx].to(device).float()\n",
        "        return states, actions, rewards, next_states, dones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_target_image(h: int, w: int, num_shapes: int = 4) -> torch.Tensor:\n",
        "    img = torch.zeros(3, h, w)\n",
        "    for _ in range(num_shapes):\n",
        "        color = torch.rand(3, 1, 1)\n",
        "        y0 = random.randint(0, h - 8)\n",
        "        x0 = random.randint(0, w - 8)\n",
        "        y1 = min(h, y0 + random.randint(6, h // 2))\n",
        "        x1 = min(w, x0 + random.randint(6, w // 2))\n",
        "        img[:, y0:y1, x0:x1] = color\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_pair_sampler(\n",
        "    image_shape: Tuple[int, int, int],\n",
        "    mirror_prob: float = 0.3,\n",
        "    angle_step: float = 5.0,\n",
        ") -> Callable[[torch.device], Tuple[torch.Tensor, torch.Tensor, bool]]:\n",
        "    _, h, w = image_shape\n",
        "\n",
        "    def sampler(device: torch.device):\n",
        "        target = random_target_image(h, w)\n",
        "        is_mirrored = random.random() < mirror_prob\n",
        "        source = torch.flip(target, dims=[2]) if is_mirrored else target.clone()\n",
        "        angle = random.choice(np.arange(0.0, 360.0, angle_step))\n",
        "        source = rotate_tensor(source, angle)\n",
        "        return source, target, is_mirrored\n",
        "\n",
        "    return sampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_action(qnet: nn.Module, state: torch.Tensor, epsilon: float, action_space) -> int:\n",
        "    if random.random() < epsilon:\n",
        "        return action_space.sample()\n",
        "    with torch.no_grad():\n",
        "        if state.dim() == 3:\n",
        "            state = state.unsqueeze(0)\n",
        "        q = qnet(state)\n",
        "        return int(q.argmax(dim=1).item())\n",
        "\n",
        "\n",
        "def train_dqn(\n",
        "    env: RotationEnv,\n",
        "    qnet: QNetwork,\n",
        "    target_net: QNetwork,\n",
        "    buffer: ReplayBuffer,\n",
        "    episodes: int = 2000,\n",
        "    batch_size: int = 64,\n",
        "    gamma: float = 0.99,\n",
        "    eps_start: float = 1.0,\n",
        "    eps_end: float = 0.05,\n",
        "    eps_decay_episodes: int = 1000,\n",
        "    target_update_steps: int = 1000,\n",
        "    lr: float = 1e-4,\n",
        "    max_steps: int = 50,\n",
        "):\n",
        "    device = next(qnet.parameters()).device\n",
        "    optimizer = torch.optim.AdamW(qnet.parameters(), lr=lr)\n",
        "    huber = nn.SmoothL1Loss()\n",
        "\n",
        "    qnet.train()\n",
        "    target_net.eval()\n",
        "\n",
        "    global_step = 0\n",
        "    rewards_history = []\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        epsilon = eps_end + (eps_start - eps_end) * max(0.0, (eps_decay_episodes - ep) / eps_decay_episodes)\n",
        "        state, _ = env.reset()\n",
        "        ep_reward = 0.0\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            action = select_action(qnet, state, epsilon, env.action_space)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            global_step += 1\n",
        "\n",
        "            if buffer.size >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size, device)\n",
        "                q_values = qnet(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "                with torch.no_grad():\n",
        "                    next_q = target_net(next_states).max(dim=1)[0]\n",
        "                    target = rewards + gamma * (1.0 - dones) * next_q\n",
        "                loss = huber(q_values, target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if global_step % target_update_steps == 0:\n",
        "                target_net.load_state_dict(qnet.state_dict())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards_history.append(ep_reward)\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"Episode {ep:4d} | eps {epsilon:.3f} | reward {ep_reward:.1f}\")\n",
        "\n",
        "    return rewards_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode   50 | eps 0.953 | reward -100.0\n",
            "Episode  100 | eps 0.905 | reward -100.1\n",
            "Episode  150 | eps 0.858 | reward -100.0\n",
            "Episode  200 | eps 0.810 | reward -100.0\n",
            "Episode  250 | eps 0.762 | reward -100.1\n",
            "Episode  300 | eps 0.715 | reward -100.0\n",
            "Episode  350 | eps 0.667 | reward -100.1\n",
            "Episode  400 | eps 0.620 | reward -100.4\n",
            "Episode  450 | eps 0.573 | reward 99.9\n",
            "Episode  500 | eps 0.525 | reward -100.0\n",
            "Episode  550 | eps 0.477 | reward 99.0\n",
            "Episode  600 | eps 0.430 | reward -100.1\n",
            "Episode  650 | eps 0.382 | reward 99.5\n",
            "Episode  700 | eps 0.335 | reward -100.0\n",
            "Episode  750 | eps 0.287 | reward -100.5\n",
            "Episode  800 | eps 0.240 | reward -100.1\n",
            "Episode  850 | eps 0.193 | reward -101.0\n",
            "Episode  900 | eps 0.145 | reward -101.9\n",
            "Episode  950 | eps 0.098 | reward -104.6\n",
            "Episode 1000 | eps 0.050 | reward -103.4\n"
          ]
        }
      ],
      "source": [
        "# Example: create env, networks, and buffer\n",
        "set_seed(0)\n",
        "\n",
        "H, W = 64, 64\n",
        "image_shape = (3, H, W)\n",
        "\n",
        "sampler = make_pair_sampler(image_shape=image_shape, mirror_prob=0.3, angle_step=5.0)\n",
        "\n",
        "env = RotationEnv(\n",
        "    image_shape=image_shape,\n",
        "    max_steps=50,\n",
        "    epsilon=0.01,\n",
        "    step_penalty=-0.1,\n",
        "    device=device,\n",
        "    pair_sampler=sampler,\n",
        ")\n",
        "\n",
        "qnet = QNetwork(input_shape=(6, H, W)).to(device)\n",
        "target_net = QNetwork(input_shape=(6, H, W)).to(device)\n",
        "target_net.load_state_dict(qnet.state_dict())\n",
        "\n",
        "buffer = ReplayBuffer(capacity=10_000, state_shape=(6, H, W), store_uint8=True)\n",
        "\n",
        "# Uncomment to train\n",
        "rewards = train_dqn(env, qnet, target_net, buffer, episodes=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rotation_episode.gif'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def render_pair_frame(source: torch.Tensor, target: torch.Tensor) -> np.ndarray:\n",
        "    # Return a side-by-side frame (H, 2W, 3) as uint8.\n",
        "    src = (source.clamp(0, 1).permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "    tgt = (target.clamp(0, 1).permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "    return np.concatenate([src, tgt], axis=1)\n",
        "\n",
        "\n",
        "def rollout_and_save_gif(\n",
        "    env: RotationEnv,\n",
        "    qnet: QNetwork,\n",
        "    out_path: str = \"rotation_episode.gif\",\n",
        "    max_steps: int = 50,\n",
        "    fps: int = 8,\n",
        "):\n",
        "    state, _ = env.reset()\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(render_pair_frame(env.current_source, env.target))\n",
        "        action = select_action(qnet, state, epsilon=0.0, action_space=env.action_space)\n",
        "        state, _, terminated, truncated, _ = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            frames.append(render_pair_frame(env.current_source, env.target))\n",
        "            break\n",
        "\n",
        "    imageio.mimsave(out_path, frames, fps=fps)\n",
        "    return out_path\n",
        "\n",
        "# Uncomment to generate a GIF (after training)\n",
        "gif_path = rollout_and_save_gif(env, qnet, out_path=\"rotation_episode.gif\")\n",
        "gif_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c08f961",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "visual-reasoning (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
