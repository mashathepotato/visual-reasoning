{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f84bb4c1",
      "metadata": {},
      "source": [
        "# TD RL (PPO) Rotation + Same/Different Decision (Chiral Shapes)\n",
        "\n",
        "This notebook uses the same **chiral shapes dataset** used in `exhaustive_search.ipynb` and other notebooks. We train a PPO agent to apply **rotation deltas** using `kornia.geometry.transform.rotate` to minimize MSE between a base shape and a target shape.\n",
        "\n",
        "After rotation, we decide if the target is the **same shape** (just rotated) or a **different shape** (mirrored then rotated), by thresholding the **best alignment error** achieved during the rollout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de52c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /Users/masha/Documents/visual-reasoning\n",
        "\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import kornia.geometry.transform as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "# Device + reproducibility\n",
        "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e73aa5f",
      "metadata": {},
      "source": [
        "## Dataset: Chiral Shapes (same as exhaustive_search / pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470e3030",
      "metadata": {},
      "outputs": [],
      "source": [
        "CHIRAL_SHAPES = {\n",
        "    'L': [(0,-1),(0,0),(0,1),(1,1)],\n",
        "    'Z': [(0,0),(-1,0),(0,1),(1,1)],\n",
        "    'S': [(0,0),(1,0),(0,1),(-1,1)],\n",
        "}\n",
        "\n",
        "def draw_shape_np(name, size=64):\n",
        "    img = np.zeros((size, size), dtype=np.uint8)\n",
        "    center = size // 2\n",
        "    block = size // 8\n",
        "    for dx, dy in CHIRAL_SHAPES[name]:\n",
        "        x = center + dx * block - block // 2\n",
        "        y = center + dy * block - block // 2\n",
        "        cv2.rectangle(img, (x, y), (x + block, y + block), 255, -1)\n",
        "    return img\n",
        "\n",
        "def norm_tensor(x):\n",
        "    return (torch.tensor(x).float().unsqueeze(0) / 255.0 - 0.5) / 0.5\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_pair(size=64, force_same=None):\n",
        "    key = random.choice(list(CHIRAL_SHAPES.keys()))\n",
        "    base_np = draw_shape_np(key, size)\n",
        "\n",
        "    angle = random.randint(0, 359)\n",
        "    M = cv2.getRotationMatrix2D((size // 2, size // 2), angle, 1.0)\n",
        "\n",
        "    if force_same is None:\n",
        "        is_same = (random.random() > 0.5)\n",
        "    else:\n",
        "        is_same = bool(force_same)\n",
        "\n",
        "    if is_same:\n",
        "        target_np = cv2.warpAffine(base_np, M, (size, size))\n",
        "        label = 1.0\n",
        "    else:\n",
        "        target_np = cv2.warpAffine(cv2.flip(base_np, 1), M, (size, size))\n",
        "        label = 0.0\n",
        "\n",
        "    base = norm_tensor(base_np).unsqueeze(0).to(DEVICE)\n",
        "    target = norm_tensor(target_np).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # Signed angle in [-180, 180]\n",
        "    angle_signed = angle if angle <= 180 else angle - 360\n",
        "\n",
        "    return base, target, label, angle_signed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78e41cb",
      "metadata": {},
      "source": [
        "## Rotation Environment\n",
        "\n",
        "- State: 2-channel tensor `[current, target]`\n",
        "- Action: rotation delta (degrees) applied with `K.rotate`\n",
        "- Reward: `prev_error - new_error` (TD-style error reduction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c7de17",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RotationEnvConfig:\n",
        "    max_steps: int = 24\n",
        "    action_scale: float = 25.0  # max absolute delta per step (deg)\n",
        "    done_threshold: float = 1e-4  # MSE threshold for success\n",
        "    size: int = 64\n",
        "    wrap_angle: bool = True\n",
        "    same_only: bool = True  # train on same pairs only\n",
        "\n",
        "\n",
        "class RotationEnv:\n",
        "    def __init__(self, cfg: RotationEnvConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = DEVICE\n",
        "        self.reset()\n",
        "\n",
        "    def _mse(self, a, b):\n",
        "        return F.mse_loss(a, b)\n",
        "\n",
        "    def _obs(self):\n",
        "        diff = self.target - self.current\n",
        "        return torch.cat([self.current, self.target, diff], dim=1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.steps = 0\n",
        "        self.base, self.target, self.label, self.angle = sample_pair(self.cfg.size, force_same=self.cfg.same_only)\n",
        "        self.current_angle = 0.0\n",
        "        self.current = self.base.clone()\n",
        "        with torch.no_grad():\n",
        "            self.prev_error = self._mse(self.current, self.target).item()\n",
        "        self.best_error = self.prev_error\n",
        "        return self._obs().detach()\n",
        "\n",
        "    def step(self, action_deg):\n",
        "        action_deg = float(max(-self.cfg.action_scale, min(self.cfg.action_scale, action_deg)))\n",
        "        self.steps += 1\n",
        "\n",
        "        self.current_angle += action_deg\n",
        "        if self.cfg.wrap_angle:\n",
        "            if self.current_angle > 180.0:\n",
        "                self.current_angle -= 360.0\n",
        "            elif self.current_angle < -180.0:\n",
        "                self.current_angle += 360.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            angle = torch.tensor([self.current_angle], device=self.device)\n",
        "            self.current = K.rotate(self.base, angle)\n",
        "            error = self._mse(self.current, self.target).item()\n",
        "\n",
        "        # Reward: relative improvement with a small action penalty\n",
        "        reward = (self.prev_error - error) / (self.prev_error + 1e-6) - 0.002 * (action_deg / self.cfg.action_scale) ** 2\n",
        "        self.prev_error = error\n",
        "        self.best_error = min(self.best_error, error)\n",
        "\n",
        "        done = (self.steps >= self.cfg.max_steps) or (error <= self.cfg.done_threshold)\n",
        "        info = {\n",
        "            \"error\": error,\n",
        "            \"best_error\": self.best_error,\n",
        "            \"label\": self.label,\n",
        "            \"angle\": self.angle,\n",
        "            \"current_angle\": self.current_angle,\n",
        "        }\n",
        "        return self._obs().detach(), reward, done, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b576225a",
      "metadata": {},
      "source": [
        "## PPO Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77c8bfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_shape):\n",
        "        super().__init__()\n",
        "        c, h, w = obs_shape\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(c, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            n = self.encoder(torch.zeros(1, c, h, w)).view(1, -1).shape[1]\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(n, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(n, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.tensor([-0.5]))\n",
        "\n",
        "    def forward(self, obs):\n",
        "        feat = self.encoder(obs).view(obs.size(0), -1)\n",
        "        mean = self.policy(feat)\n",
        "        value = self.value(feat)\n",
        "        std = self.log_std.exp().expand_as(mean)\n",
        "        return mean, std, value\n",
        "\n",
        "\n",
        "def sample_action(model, obs, action_scale):\n",
        "    mean, std, value = model(obs)\n",
        "    dist = Normal(mean, std)\n",
        "    raw_action = dist.rsample()\n",
        "    action = torch.tanh(raw_action) * action_scale\n",
        "\n",
        "    log_prob = dist.log_prob(raw_action)\n",
        "    log_prob -= math.log(action_scale)\n",
        "    log_prob -= torch.log(1 - torch.tanh(raw_action) ** 2 + 1e-6)\n",
        "    log_prob = log_prob.sum(-1)\n",
        "\n",
        "    return action, raw_action, log_prob, value\n",
        "\n",
        "@torch.no_grad()\n",
        "def deterministic_action(model, obs, action_scale):\n",
        "    mean, std, value = model(obs)\n",
        "    action = torch.tanh(mean) * action_scale\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c136eaac",
      "metadata": {},
      "source": [
        "## PPO Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533887c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "cfg = RotationEnvConfig(same_only=True)\n",
        "steps_per_rollout = 256\n",
        "ppo_epochs = 10\n",
        "mini_batch_size = 64\n",
        "clip_eps = 0.2\n",
        "gamma = 0.99\n",
        "lam = 0.95\n",
        "value_coef = 0.5\n",
        "entropy_coef = 0.01\n",
        "lr = 5e-4\n",
        "updates = 60\n",
        "\n",
        "# Setup\n",
        "env = RotationEnv(cfg)\n",
        "obs_shape = env.reset().shape[1:]\n",
        "model = ActorCritic(obs_shape).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def compute_gae(rewards, values, dones, next_value):\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    gae = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        mask = 1.0 - dones[t]\n",
        "        delta = rewards[t] + gamma * next_value * mask - values[t]\n",
        "        gae = delta + gamma * lam * mask * gae\n",
        "        advantages[t] = gae\n",
        "        next_value = values[t]\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n",
        "\n",
        "all_returns = []\n",
        "obs = env.reset()\n",
        "\n",
        "for update in range(updates):\n",
        "    obs_buf = []\n",
        "    raw_action_buf = []\n",
        "    logp_buf = []\n",
        "    reward_buf = []\n",
        "    done_buf = []\n",
        "    value_buf = []\n",
        "\n",
        "    for step in range(steps_per_rollout):\n",
        "        action, raw_action, logp, value = sample_action(model, obs, cfg.action_scale)\n",
        "        next_obs, reward, done, info = env.step(action.item())\n",
        "\n",
        "        obs_buf.append(obs)\n",
        "        # Detach rollout tensors to avoid backprop through old graphs\n",
        "        raw_action_buf.append(raw_action.detach())\n",
        "        logp_buf.append(logp.detach())\n",
        "        value_buf.append(value.squeeze(0).detach())\n",
        "        reward_buf.append(torch.tensor(reward, dtype=torch.float32, device=DEVICE))\n",
        "        done_buf.append(torch.tensor(float(done), device=DEVICE))\n",
        "\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            all_returns.append(-info[\"best_error\"])\n",
        "            obs = env.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, _, next_value = model(obs)\n",
        "\n",
        "    rewards = torch.stack(reward_buf)\n",
        "    values = torch.stack(value_buf).squeeze(-1)\n",
        "    dones = torch.stack(done_buf)\n",
        "    advantages, returns = compute_gae(rewards, values, dones, next_value.item())\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    obs_batch = torch.cat(obs_buf, dim=0)\n",
        "    raw_action_batch = torch.cat(raw_action_buf, dim=0)\n",
        "    old_logp_batch = torch.cat(logp_buf, dim=0)\n",
        "    returns_batch = returns.detach()\n",
        "    adv_batch = advantages.detach()\n",
        "\n",
        "    idxs = torch.randperm(steps_per_rollout)\n",
        "    for _ in range(ppo_epochs):\n",
        "        for start in range(0, steps_per_rollout, mini_batch_size):\n",
        "            mb_idx = idxs[start:start + mini_batch_size]\n",
        "            mb_obs = obs_batch[mb_idx]\n",
        "            mb_raw_action = raw_action_batch[mb_idx]\n",
        "            mb_old_logp = old_logp_batch[mb_idx]\n",
        "            mb_returns = returns_batch[mb_idx]\n",
        "            mb_adv = adv_batch[mb_idx]\n",
        "\n",
        "            mean, std, value = model(mb_obs)\n",
        "            dist = Normal(mean, std)\n",
        "\n",
        "            logp = dist.log_prob(mb_raw_action)\n",
        "            logp -= math.log(cfg.action_scale)\n",
        "            logp -= torch.log(1 - torch.tanh(mb_raw_action) ** 2 + 1e-6)\n",
        "            logp = logp.sum(-1)\n",
        "\n",
        "            ratio = torch.exp(logp - mb_old_logp)\n",
        "            surr1 = ratio * mb_adv\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * mb_adv\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            value_loss = F.mse_loss(value.squeeze(-1), mb_returns)\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    if (update + 1) % 5 == 0:\n",
        "        print(f\"Update {update+1:02d} | recent returns: {all_returns[-5:]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e9338cf",
      "metadata": {},
      "source": [
        "## Training Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54df5d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(all_returns)\n",
        "plt.title(\"Episode Returns (higher is better)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return proxy\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cdd1922",
      "metadata": {},
      "source": [
        "## Evaluate: Alignment + Same/Different Decision\n",
        "\n",
        "We compute the **best alignment MSE** achieved during a rollout. For classification, we use two rules:\n",
        "\n",
        "1) **Threshold rule**: if best MSE is below a calibrated threshold, predict \"same\".\n",
        "2) **Mirror-contrast rule**: run the policy on both the base and the mirrored base, then pick the one with lower best MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a81dec",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mse_err(a, b):\n",
        "    return torch.mean((a - b) ** 2).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollout_best_error_pair(model, base, target, cfg, deterministic=True):\n",
        "    current_angle = 0.0\n",
        "    current = base\n",
        "    best_err = mse_err(current, target)\n",
        "\n",
        "    obs = torch.cat([current, target, target - current], dim=1)\n",
        "    for _ in range(cfg.max_steps):\n",
        "        if deterministic:\n",
        "            action = deterministic_action(model, obs, cfg.action_scale)\n",
        "        else:\n",
        "            action, _, _, _ = sample_action(model, obs, cfg.action_scale)\n",
        "\n",
        "        action_deg = float(max(-cfg.action_scale, min(cfg.action_scale, action.item())))\n",
        "        current_angle += action_deg\n",
        "        if cfg.wrap_angle:\n",
        "            if current_angle > 180.0:\n",
        "                current_angle -= 360.0\n",
        "            elif current_angle < -180.0:\n",
        "                current_angle += 360.0\n",
        "\n",
        "        current = K.rotate(base, torch.tensor([current_angle], device=base.device))\n",
        "        err = mse_err(current, target)\n",
        "        best_err = min(best_err, err)\n",
        "\n",
        "        obs = torch.cat([current, target, target - current], dim=1)\n",
        "\n",
        "    return best_err\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_metrics(model, n=300):\n",
        "    errs = []\n",
        "    errs_mir = []\n",
        "    labels = []\n",
        "    scores = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        base, target, label, _ = sample_pair(force_same=None)\n",
        "        err_base = rollout_best_error_pair(model, base, target, cfg, deterministic=True)\n",
        "\n",
        "        base_mir = torch.flip(base, dims=[3])\n",
        "        err_mir = rollout_best_error_pair(model, base_mir, target, cfg, deterministic=True)\n",
        "\n",
        "        errs.append(err_base)\n",
        "        errs_mir.append(err_mir)\n",
        "        labels.append(label)\n",
        "        scores.append(err_mir - err_base)  # higher => same\n",
        "\n",
        "    return np.array(errs), np.array(errs_mir), np.array(labels), np.array(scores)\n",
        "\n",
        "# Calibration set to pick a threshold\n",
        "calib_errs, calib_errs_mir, calib_labels, calib_scores = collect_metrics(model, n=200)\n",
        "mean_same = calib_errs[calib_labels == 1.0].mean()\n",
        "mean_diff = calib_errs[calib_labels == 0.0].mean()\n",
        "THRESHOLD = (mean_same + mean_diff) / 2.0\n",
        "\n",
        "# Test set\n",
        "errs, errs_mir, labels, scores = collect_metrics(model, n=200)\n",
        "\n",
        "# Rule 1: threshold on best MSE\n",
        "preds_thresh = (errs < THRESHOLD).astype(float)\n",
        "acc_thresh = (preds_thresh == labels).mean()\n",
        "auc_thresh = metrics.roc_auc_score(labels, -errs)\n",
        "\n",
        "# Rule 2: mirror-contrast\n",
        "preds_cmp = (errs < errs_mir).astype(float)\n",
        "acc_cmp = (preds_cmp == labels).mean()\n",
        "auc_cmp = metrics.roc_auc_score(labels, scores)\n",
        "\n",
        "print(f\"Threshold (MSE): {THRESHOLD:.6f}\")\n",
        "print(f\"Accuracy (threshold): {acc_thresh*100:.2f}% | AUC: {auc_thresh:.4f}\")\n",
        "print(f\"Accuracy (mirror-contrast): {acc_cmp*100:.2f}% | AUC: {auc_cmp:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(errs[labels == 1.0], bins=20, alpha=0.7, color='green', label='Same')\n",
        "plt.hist(errs[labels == 0.0], bins=20, alpha=0.7, color='red', label='Different')\n",
        "plt.axvline(THRESHOLD, color='black', linestyle='--', label='Threshold')\n",
        "plt.title(\"Best Alignment Error Distribution\")\n",
        "plt.xlabel(\"Best MSE (lower is better)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(scores[labels == 1.0], bins=20, alpha=0.7, color='green', label='Same')\n",
        "plt.hist(scores[labels == 0.0], bins=20, alpha=0.7, color='red', label='Different')\n",
        "plt.axvline(0.0, color='black', linestyle='--', label='Zero')\n",
        "plt.title(\"Mirror-Contrast Score (higher => same)\")\n",
        "plt.xlabel(\"Score = best_err_mir - best_err\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b91015",
      "metadata": {},
      "source": [
        "## Visualize a Few Decisions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ed4940",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_and_visualize(n=3):\n",
        "    for i in range(n):\n",
        "        obs = env.reset()\n",
        "        best_err = env.best_error\n",
        "        for _ in range(env.cfg.max_steps):\n",
        "            action = deterministic_action(model, obs, env.cfg.action_scale)\n",
        "            obs, reward, done, info = env.step(action.item())\n",
        "            best_err = min(best_err, info[\"error\"])\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        pred = 1.0 if best_err < THRESHOLD else 0.0\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
        "        axes[0].imshow(env.base[0,0].cpu(), cmap='gray')\n",
        "        axes[0].set_title(\"Base\")\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        axes[1].imshow(env.target[0,0].cpu(), cmap='gray')\n",
        "        axes[1].set_title(f\"Target (label {int(env.label)})\")\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        axes[2].imshow(env.current[0,0].cpu(), cmap='gray')\n",
        "        axes[2].set_title(f\"Final | pred {int(pred)} | best MSE {best_err:.4f}\")\n",
        "        axes[2].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "run_and_visualize(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e759067",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "visual-reasoning (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
