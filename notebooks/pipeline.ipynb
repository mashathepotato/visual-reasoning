{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aaeea05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/masha/Documents/visual-reasoning\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/masha/Documents/visual-reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e42ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masha/Documents/visual-reasoning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import timm\n",
    "\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "MODEL_PATH = 'models/rotator_l1_100e_10k.pth'\n",
    "print('DEVICE:', DEVICE)\n",
    "# DINO backbone for rotator conditioning\n",
    "dino = timm.create_model('vit_small_patch16_dinov3', pretrained=True).to(DEVICE).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_dino_embedding(img_64_tensor):\n",
    "    img = (img_64_tensor * 0.5) + 0.5\n",
    "    img = F.interpolate(img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    img = img.repeat(1, 3, 1, 1)\n",
    "    norm = torch.nn.functional.normalize\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1,3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1,3,1,1)\n",
    "    img = (img - mean) / std\n",
    "    emb = dino.forward_features(img)[:, 0, :]\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ac3f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIRAL_SHAPES = {\n",
    "    'L': [(0,-1),(0,0),(0,1),(1,1)],\n",
    "    'Z': [(0,0),(-1,0),(0,1),(1,1)],\n",
    "    'S': [(0,0),(1,0),(0,1),(-1,1)],\n",
    "}\n",
    "\n",
    "def draw_shape_np(name, size=64):\n",
    "    img = np.zeros((size, size), dtype=np.uint8)\n",
    "    center = size // 2\n",
    "    block = size // 8\n",
    "    for dx, dy in CHIRAL_SHAPES[name]:\n",
    "        x = center + dx * block - block // 2\n",
    "        y = center + dy * block - block // 2\n",
    "        cv2.rectangle(img, (x, y), (x + block, y + block), 255, -1)\n",
    "    return img\n",
    "\n",
    "def norm_tensor(x):\n",
    "    return (torch.tensor(x).float().unsqueeze(0) / 255.0 - 0.5) / 0.5\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, n=1000):\n",
    "        self.n = n\n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, idx):\n",
    "        key = random.choice(list(CHIRAL_SHAPES.keys()))\n",
    "        base_np = draw_shape_np(key, 64)\n",
    "        angle = random.randint(0, 359)\n",
    "        M = cv2.getRotationMatrix2D((32, 32), angle, 1.0)\n",
    "        is_same = (random.random() > 0.5)\n",
    "        if is_same:\n",
    "            target_np = cv2.warpAffine(base_np, M, (64, 64))\n",
    "            label = 1.0\n",
    "        else:\n",
    "            target_np = cv2.warpAffine(cv2.flip(base_np, 1), M, (64, 64))\n",
    "            label = 0.0\n",
    "        return norm_tensor(base_np), norm_tensor(target_np), torch.tensor([label]).float()\n",
    "\n",
    "train_loader = DataLoader(PairDataset(2000), batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(PairDataset(400), batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92697732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rotator (same as exhaustive search) ---\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class FastRotator(nn.Module):\n",
    "    def __init__(self, backbone_dim=384, flow_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(nn.Linear(1, flow_dim*4), nn.GELU(), nn.Linear(flow_dim*4, flow_dim*4))\n",
    "        self.angle_mlp = nn.Sequential(nn.Linear(1, flow_dim*4), nn.GELU(), nn.Linear(flow_dim*4, flow_dim*4))\n",
    "        self.cond_proj = nn.Linear(backbone_dim, flow_dim*4)\n",
    "\n",
    "        self.inc = DoubleConv(1, flow_dim)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(flow_dim, flow_dim*2))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(flow_dim*2, flow_dim*4))\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv1 = DoubleConv(flow_dim*6, flow_dim*2)\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv2 = DoubleConv(flow_dim*3, flow_dim)\n",
    "\n",
    "        self.outc = nn.Conv2d(flow_dim, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x_t, t, dino_emb, target_angle_deg):\n",
    "        t_emb = self.time_mlp(t)\n",
    "        d_emb = self.cond_proj(dino_emb)\n",
    "        a_emb = self.angle_mlp(target_angle_deg / 360.0)\n",
    "        global_cond = (t_emb + d_emb + a_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        x1 = self.inc(x_t)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x3 = x3 + global_cond\n",
    "\n",
    "        x = self.conv1(torch.cat([self.up1(x3), x2], dim=1))\n",
    "        x = self.conv2(torch.cat([self.up2(x), x1], dim=1))\n",
    "        return self.outc(x)\n",
    "\n",
    "rotator = FastRotator().to(DEVICE)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(MODEL_PATH)\n",
    "rotator.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "rotator.eval()\n",
    "\n",
    "def apply_rotation(model, base_img, base_emb, angle_deg, steps=10):\n",
    "    model.eval()\n",
    "    dt = 1.0 / steps\n",
    "    curr = base_img.clone()\n",
    "    B = base_img.shape[0]\n",
    "    if torch.is_tensor(angle_deg):\n",
    "        target_ang = angle_deg.to(DEVICE)\n",
    "        if target_ang.ndim == 1:\n",
    "            target_ang = target_ang.view(-1, 1)\n",
    "    else:\n",
    "        target_ang = torch.full((B, 1), angle_deg, device=DEVICE)\n",
    "    for i in range(steps):\n",
    "        t = torch.full((B, 1), i/steps, device=DEVICE)\n",
    "        v = model(curr, t, base_emb, target_ang)\n",
    "        curr = curr + v * dt\n",
    "    return curr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1fdd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reasoning model: encoder + controller + rotator-in-the-loop ---\n",
    "class TinyEncoder(nn.Module):\n",
    "    def __init__(self, in_ch=1, dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, dim, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(dim, dim, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(dim, dim, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    def __init__(self, dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*2, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, h_state, h_target):\n",
    "        return self.mlp(torch.cat([h_state, h_target], dim=-1))\n",
    "\n",
    "class Reasoner(nn.Module):\n",
    "    def __init__(self, steps=8, enc_dim=64):\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "        self.encoder = TinyEncoder(dim=enc_dim)\n",
    "        self.controller = Controller(dim=enc_dim)\n",
    "        self.head = nn.Sequential(nn.Linear(enc_dim, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, img_a, img_b, return_traj=False):\n",
    "        # Encode target once\n",
    "        h_target = self.encoder(img_b)\n",
    "\n",
    "        # Initialize state\n",
    "        curr = img_a\n",
    "        h_state = self.encoder(curr)\n",
    "\n",
    "        states = [curr]\n",
    "        deltas = []\n",
    "        h_states = [h_state]\n",
    "\n",
    "        # Use DINO identity embedding for the rotator\n",
    "        rot_emb = get_dino_embedding(img_a).detach()\n",
    "\n",
    "        # Unroll rotation reasoning\n",
    "        for _ in range(self.steps):\n",
    "            delta = self.controller(h_state, h_target)\n",
    "            delta = torch.clamp(delta, -30.0, 30.0)\n",
    "\n",
    "            curr = apply_rotation(rotator, curr, rot_emb, delta, steps=10)\n",
    "            h_state = self.encoder(curr)\n",
    "\n",
    "            states.append(curr)\n",
    "            deltas.append(delta)\n",
    "            h_states.append(h_state)\n",
    "\n",
    "        logits = self.head(h_state)\n",
    "\n",
    "        if return_traj:\n",
    "            return logits, states, deltas, h_states, h_target\n",
    "        return logits\n",
    "\n",
    "reasoner = Reasoner(steps=8).to(DEVICE)\n",
    "opt = torch.optim.Adam(reasoner.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da167600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.693809\n",
      "Epoch 2: loss=0.693219\n",
      "Epoch 3: loss=0.693424\n",
      "Epoch 4: loss=0.693312\n",
      "Epoch 5: loss=0.693424\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    reasoner.train()\n",
    "    total = 0.0\n",
    "    for img_a, img_b, label in train_loader:\n",
    "        img_a = img_a.to(DEVICE)\n",
    "        img_b = img_b.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        logits = reasoner(img_a, img_b)\n",
    "        loss = loss_fn(logits, label)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    print(f'Epoch {epoch+1}: loss={total/len(train_loader):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "297d2a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.25%\n",
      "AUC: 0.5061\n"
     ]
    }
   ],
   "source": [
    "# --- Eval ---\n",
    "reasoner.eval()\n",
    "y_true = []\n",
    "y_scores = []\n",
    "with torch.no_grad():\n",
    "    for img_a, img_b, label in test_loader:\n",
    "        img_a = img_a.to(DEVICE)\n",
    "        img_b = img_b.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        logits = reasoner(img_a, img_b)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        y_true.extend(label.cpu().numpy().flatten().tolist())\n",
    "        y_scores.extend(probs.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_scores = np.array(y_scores)\n",
    "y_pred = (y_scores >= 0.5).astype(float)\n",
    "acc = (y_pred == y_true).mean()\n",
    "auc = metrics.roc_auc_score(y_true, y_scores)\n",
    "print(f'Accuracy: {acc*100:.2f}%')\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dab2d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     39\u001b[39m     target_np = cv2.warpAffine(cv2.flip(base_np, \u001b[32m1\u001b[39m), M, (\u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m))\n\u001b[32m     41\u001b[39m target = norm_tensor(target_np).unsqueeze(\u001b[32m0\u001b[39m).to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mvisualize_reasoning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreasoner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/visual-reasoning/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mvisualize_reasoning\u001b[39m\u001b[34m(reasoner, img_a, img_b, steps)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_reasoning\u001b[39m(reasoner, img_a, img_b, steps=\u001b[32m8\u001b[39m):\n\u001b[32m      4\u001b[39m     reasoner.eval()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     logits, states, deltas = reasoner(img_a, img_b, return_traj=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m     prob = torch.sigmoid(logits).item()\n\u001b[32m      8\u001b[39m     cols = \u001b[38;5;28mlen\u001b[39m(states) + \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_reasoning(reasoner, img_a, img_b, steps=8):\n",
    "    reasoner.eval()\n",
    "    logits, states, deltas = reasoner(img_a, img_b, return_traj=True)\n",
    "    prob = torch.sigmoid(logits).item()\n",
    "\n",
    "    cols = len(states) + 1\n",
    "    fig, axes = plt.subplots(1, cols, figsize=(3 * cols, 3))\n",
    "\n",
    "    axes[0].imshow(img_a[0,0].cpu(), cmap='gray')\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    for i in range(1, len(states)):\n",
    "        axes[i].imshow(states[i][0,0].cpu(), cmap='gray')\n",
    "        d = float(deltas[i-1].detach().cpu())\n",
    "        axes[i].set_title(f'Step {i}\u0394 {d:.2f} deg')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    axes[-1].imshow(img_b[0,0].cpu(), cmap='gray')\n",
    "    axes[-1].set_title(f'Target P(same)={prob:.2f}')\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sample a pair\n",
    "key = random.choice(list(CHIRAL_SHAPES.keys()))\n",
    "base_np = draw_shape_np(key, 64)\n",
    "base = norm_tensor(base_np).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "angle = random.randint(0, 359)\n",
    "M = cv2.getRotationMatrix2D((32, 32), angle, 1.0)\n",
    "\n",
    "is_same = (random.random() > 0.5)\n",
    "if is_same:\n",
    "    target_np = cv2.warpAffine(base_np, M, (64, 64))\n",
    "else:\n",
    "    target_np = cv2.warpAffine(cv2.flip(base_np, 1), M, (64, 64))\n",
    "\n",
    "target = norm_tensor(target_np).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "visualize_reasoning(reasoner, base, target, steps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual-reasoning (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}